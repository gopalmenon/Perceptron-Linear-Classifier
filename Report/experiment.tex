\section{The Perceptron Algorithm and Its Variants}

\subsection{The Task and Data}

\subsection{Algorithms}

\subsection{Experiments}

\begin{enumerate}
\item[1.] The weight vector at the end of the run was $\left [0.0, 0.0, 1.0, 0.0, -1.0, 2.0 \right ] ^ T$. The number of mistakes made was $4$.
  
\item[2.] 6-fold cross validation was run for finding the hyper-parameters for the Perceptron and Margin Perceptron. 

For the Perceptron, a learning rate of $0.2$ was selected after cross-validation. $1382$ mistakes were made during the training process. The accuracy on the training set (calculated in all cases of this assignment by adding true positives and true negatives and dividing by total number of training samples) was $0.7989$, and was $0.7941$ on the testing set.

For the Margin Perceptron, a learning rate of $0.1$ and a $\mu$ value of $5.0$ was selected after cross-validation. $2431$ mistakes were made during the training process. The accuracy on the training set was $0.8458$, and was $0.8413$ on the testing set.

\item[3.] In this case, the data was shuffled for the case of multiple epochs before each subsequent epoch.

For the Perceptron, a learning rate of $0.6$ and $5$ epochs was selected after cross-validation. $6600$ mistakes were made during the training process. The accuracy on the training set was $0.7957$, and was $0.7984$ on the testing set.

For the Margin Perceptron, a learning rate of $0.2$, a $\mu$ value of $3.0$ and $3$ epochs was selected after cross-validation. $5903$ mistakes were made during the training process. The accuracy on the training set was $0.8388$, and was $0.8343$ on the testing set.

In both cases, the hyper-parameters selected when running over multiple epochs resulted in reduced accuracy over both training and testing sets. This is possibly because the samples are not linearly separated and the separating hyperplane results in classification errors. Additional updates to weights will not increase accuracy and the accuracy will tend to be more or less the same. The hyperplane may shift a little with each update, but does not converge beyond a given point.

\item[4.] When the Aggressive Margin Perceptron was trained for $1$ epoch, cross-validation resulted in a $\mu$ of $1.0$. $2584$ mistakes were made during the training process. The accuracy on the training set was $0.7919$, and was $0.7904$ on the testing set.

When the number of epochs were also selected by cross-validation, the Aggressive Margin Perceptron used a $\mu$ of $2.0$ and $3$ epochs were selected. $7795$ mistakes were made during the training process. The accuracy on the training set was $0.8240$, and was $0.8269$ on the testing set. The data was shuffled in this case of multiple epochs. The shuffling was done before each subsequent epoch.


\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw"
%%% End:
