\section{Mistake Bound Model of Learning}
\begin{enumerate}


\item[1.] Each function $f_r$ in the concept class $\mathcal{C}$ is defined by a radius $r$. Since $1 \leq r \leq 80$ and $r$ is being compared with the sum of the squares of two integers, we need only consider integral values of $r$. So each function  $f_r$ in the concept class $\mathcal{C}$ that needs to be considered, will have a different integral value of $r$. So $|\mathcal{C}| = 80$.
  
\item[2.] [5 points] We need to check if the following equality is true

\begin{equation*}
y^t = \left\{
    \begin{array}{rl}
      +1 & (x_1^t)^2 + (x_2^t)^2 \leq r^2;\\
      -1 & \mbox{otherwise}
    \end{array}
\right.
\label{eq:f_r}
\end{equation*}

If it is not true then it means that the hypothesis $f_r$ has made a mistake.

\item[3.] [10 points] Consider the case when the label is $-1$ and the prediction is $+1$ because $x_1^2 + x_2^2 \leq r^2$. In order to correct this, we need to update $r$ to make it $x_1^2 + x_2^2 > r^2$ or $r = \floor*{\sqrt{x_1^2 + x_2^2 - 1}}$.\\

Consider the case when the label is $+1$ and the prediction is $-1$ because $x_1^2 + x_2^2 > r^2$. In order to correct this, we need to update $r$ to make it $r = \ceil*{\sqrt{x_1^2 + x_2^2 + 1}}$.\\

In both cases above, we need to consider only the positive value of the square root.

\item[4.] [20 points] Here is a mistake-driven learning algorithm to learn the function.

\begin{minipage}{\linewidth}
  \begin{algorithm}[H]
    \caption{Mistake-Driven Learning Algorithm}\label{MDLA}
    \begin{algorithmic}[1]
      \Procedure{Mistake-Driven Learning Algorithm}{$x_1, x_2, y$}
	\If {$x_1^2 + x_2^2 \leq r^2$}
	  \If {y == $-1$}
	    \State $r = \floor*{\sqrt{x_1^2 + x_2^2 - 1}}$
	  \EndIf
	\Else 
	  \If {y == $+1$}
	   \State $r = \ceil*{\sqrt{x_1^2 + x_2^2 + 1}}$
	  \EndIf
	\EndIf
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{minipage}\\

Here the algorithm receives as input the values of $x_1$, $x_2$ and the label $y$. It then uses these values to update the value of $r$ that it maintains in its internal state. In the algorithm above, $==$ represents the test for equals and $=$ represents an assignment.

Since the correct function will use a value of $r$ between $1$ and $80$, the worst case scenario for learning the correct function will be the case where all the functions with the incorrect value of $r$ are first tried and the test data results in a wrong prediction in each such case. So the correct function will be the last one tried and will be found after making 79 (that is $\left | \mathcal{C} - 1 \right |$) mistakes. 

\item[5.] 
  \begin{enumerate}
  \item[a.] The set of hypotheses consistent with all examples seen so far can be defined by storing the upper and lower values of the range of $r$ values that satisfy the examples seen so far.
  \item[b.] [5 points] At any point in the iteration of the halving algorithm, we can check and see if the value of $r^2$ corresponding to the lowest value of $r$ in the range of $r$ values in the top half of the ranges of $r$ satisfies the following
   \begin{equation*}
y^t = \left\{
    \begin{array}{rl}
      +1 & (x_1^t)^2 + (x_2^t)^2 \leq r^2;\\
      -1 & \mbox{otherwise}
    \end{array}
\right.
\label{eq:f_r}
\end{equation*}

  \item[c.] [5 points] The halving algorithm can be as follows:
  
\floatevery{algorithm}{\setlength\hsize{15cm}}
\begin{minipage}{\linewidth}
  \begin{algorithm}[H]
    \caption{Halving Algorithm}\label{MDLA}
    \begin{algorithmic}[1]
      \Procedure{Halving Algorithm}{$x_1^t, x_2^t, y^t$}
	\State  \parbox[t]{\dimexpr\linewidth-\algorithmicindent} {Construct sets $R_1$ and $R_2$ by splitting the sorted set $R$ of remaining $r$ values down the middle. The split is made such that for the case of odd number of $r$ values, the set $R_2$ will contain one more element than $R_1$}
	\If {$\left | R_1 \right | ==  \left | R_2 \right | $ and $\left | R_1 \right | \neq 1$}
	 \State Remove largest vaue of $r$ from $R_1$ and put it into $R_2$ \label{lst:line:majority}
	\EndIf
	\State $r_t =$ minimum value of r in set $R_2$
	\If {$(x_1^t)^2 + (x_2^t)^2 \leq r_t^2$ and $y^t == -1$} \label{lst:line:majorityWrong}
	  \State $R = R_1$
	  \If {$\left | R \right | == 1$}
	    \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent} {The function has been learnt and is $f_r$ where $r =$ the element in set $R$}
	  \EndIf
	\EndIf
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
\end{minipage}\\

In the above halving algorithm, in step~\ref{lst:line:majority}, the set $R_2$ is made the majority set if it is not already one. In the case where there is only one element left in each split set, there is no change made. The majority step is used to make a prediction. If the prediction is wrong, the entire set is dropped from the list of potential values of $r$ that will be considered to be the value used in the target function. This step where the majority set is checked is shown in line~\ref{lst:line:majorityWrong}.\\

The halving algorithm discards at least half the functions from the hypothesis set each time its makes a mistake in the prediction. In the worst case, the algorithm discards exactly half the functions from the hypothesis set. This means that it uses at most $log_2 \left | \mathcal{C} \right |$ steps to arrive at the correct function. Here $\mathcal{C}$ is the concept class that the algorithm searches over. This means that the mistake bound (the number of steps in the worst case) is $log_2 80$.
  \end{enumerate}

\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw"
%%% End:
